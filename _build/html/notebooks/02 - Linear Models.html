
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 2: Linear models &#8212; ML Engineering</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 3: Kernelization" href="03%20-%20Kernelization.html" />
    <link rel="prev" title="Lecture 1: Introduction" href="01%20-%20Introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/banner.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">ML Engineering</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Overview
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01%20-%20Introduction.html">
   Lecture 1: Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lecture 2: Linear models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03%20-%20Kernelization.html">
   Lecture 3: Kernelization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04%20-%20Model%20Selection.html">
   Lecture 4: Model Selection
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/02 - Linear Models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ml-course/master"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ml-course/master/issues/new?title=Issue%20on%20page%20%2Fnotebooks/02 - Linear Models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ml-course/master/master?urlpath=tree/notebooks/02 - Linear Models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/ml-course/master/blob/master/notebooks/02 - Linear Models.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 2: Linear models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notation-and-definitions">
     Notation and Definitions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#basic-operations">
       Basic operations
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gradients">
       Gradients
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#distributions-and-probabilities">
       Distributions and Probabilities
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-models">
   Linear models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-for-regression">
     Linear models for regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-regression-aka-ordinary-least-squares">
       Linear Regression (aka Ordinary Least Squares)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#solving-ordinary-least-squares">
         Solving ordinary least squares
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#gradient-descent">
         Gradient Descent
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">
         Stochastic Gradient Descent (SGD)
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#in-practice">
         In practice
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ridge-regression">
       Ridge regression
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id1">
         In practice
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-ways-to-reduce-overfitting">
       Other ways to reduce overfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lasso-least-absolute-shrinkage-and-selection-operator">
       Lasso (Least Absolute Shrinkage and Selection Operator)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#coordinate-descent">
         Coordinate descent
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#coordinate-descent-with-lasso">
         Coordinate descent with Lasso
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interpreting-l1-and-l2-loss">
       Interpreting L1 and L2 loss
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#elastic-net">
       Elastic-Net
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-loss-functions-for-regression">
       Other loss functions for regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-for-classification">
     Linear models for Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logistic-regression">
       Logistic regression
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#loss-function-cross-entropy">
         Loss function: Cross-entropy
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#optimization-methods-solvers-for-cross-entropy-loss">
         Optimization methods (solvers) for cross-entropy loss
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id2">
         In practice
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ridge-classification">
       Ridge Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#support-vector-machines">
       Support vector machines
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#solving-svms-with-lagrange-multipliers">
         Solving SVMs with Lagrange Multipliers
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#geometric-interpretation">
           Geometric interpretation
          </a>
         </li>
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#solution">
           Solution
          </a>
         </li>
        </ul>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#making-predictions">
         Making predictions
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#svms-and-knn">
           SVMs and kNN
          </a>
         </li>
        </ul>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#regularized-soft-margin-svms">
         Regularized (soft margin) SVMs
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#least-squares-svms">
         Least Squares SVMs
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#effect-of-regularization-on-margin-and-support-vectors">
         Effect of regularization on margin and support vectors
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#svms-in-scikit-learn">
         SVMs in scikit-learn
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#solving-svms-with-gradient-descent">
         Solving SVMs with Gradient Descent
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#generalized-svms">
         Generalized SVMs
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#perceptron">
       Perceptron
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-for-multiclass-classification">
     Linear Models for multiclass classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-vs-rest-aka-one-vs-all">
       one-vs-rest (aka one-vs-all)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-vs-one">
       one-vs-one
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-overview">
     Linear models overview
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lecture 2: Linear models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Lecture 2: Linear models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notation-and-definitions">
     Notation and Definitions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#basic-operations">
       Basic operations
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gradients">
       Gradients
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#distributions-and-probabilities">
       Distributions and Probabilities
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-models">
   Linear models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-for-regression">
     Linear models for regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-regression-aka-ordinary-least-squares">
       Linear Regression (aka Ordinary Least Squares)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#solving-ordinary-least-squares">
         Solving ordinary least squares
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#gradient-descent">
         Gradient Descent
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">
         Stochastic Gradient Descent (SGD)
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#in-practice">
         In practice
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ridge-regression">
       Ridge regression
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id1">
         In practice
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-ways-to-reduce-overfitting">
       Other ways to reduce overfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lasso-least-absolute-shrinkage-and-selection-operator">
       Lasso (Least Absolute Shrinkage and Selection Operator)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#coordinate-descent">
         Coordinate descent
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#coordinate-descent-with-lasso">
         Coordinate descent with Lasso
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interpreting-l1-and-l2-loss">
       Interpreting L1 and L2 loss
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#elastic-net">
       Elastic-Net
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-loss-functions-for-regression">
       Other loss functions for regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-for-classification">
     Linear models for Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logistic-regression">
       Logistic regression
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#loss-function-cross-entropy">
         Loss function: Cross-entropy
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#optimization-methods-solvers-for-cross-entropy-loss">
         Optimization methods (solvers) for cross-entropy loss
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id2">
         In practice
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ridge-classification">
       Ridge Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#support-vector-machines">
       Support vector machines
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#solving-svms-with-lagrange-multipliers">
         Solving SVMs with Lagrange Multipliers
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#geometric-interpretation">
           Geometric interpretation
          </a>
         </li>
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#solution">
           Solution
          </a>
         </li>
        </ul>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#making-predictions">
         Making predictions
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#svms-and-knn">
           SVMs and kNN
          </a>
         </li>
        </ul>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#regularized-soft-margin-svms">
         Regularized (soft margin) SVMs
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#least-squares-svms">
         Least Squares SVMs
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#effect-of-regularization-on-margin-and-support-vectors">
         Effect of regularization on margin and support vectors
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#svms-in-scikit-learn">
         SVMs in scikit-learn
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#solving-svms-with-gradient-descent">
         Solving SVMs with Gradient Descent
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#generalized-svms">
         Generalized SVMs
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#perceptron">
       Perceptron
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-for-multiclass-classification">
     Linear Models for multiclass classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-vs-rest-aka-one-vs-all">
       one-vs-rest (aka one-vs-all)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-vs-one">
       one-vs-one
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-overview">
     Linear models overview
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Auto-setup when running on Google Colab</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">())</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;/content/master&#39;</span><span class="p">):</span>
    <span class="o">!</span>git clone -q https://github.com/ML-course/master.git /content/master
    <span class="o">!</span>pip install -rq master/requirements_colab.txt
    <span class="o">%</span><span class="k">cd</span> master/notebooks

<span class="c1"># Global imports and settings</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">preamble</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">interactive</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Set to True for interactive plots</span>
<span class="k">if</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">fig_scale</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">fig_scale</span> <span class="o">=</span> <span class="mf">1.2</span>
</pre></div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="lecture-2-linear-models">
<h1>Lecture 2: Linear models<a class="headerlink" href="#lecture-2-linear-models" title="Permalink to this headline">¶</a></h1>
<p><strong>Basics of modeling, optimization, and regularization</strong></p>
<p>Joaquin Vanschoren</p>
<div class="section" id="notation-and-definitions">
<h2>Notation and Definitions<a class="headerlink" href="#notation-and-definitions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A <em>scalar</em> is a simple numeric value, denoted by an italic letter: <span class="math notranslate nohighlight">\(x=3.24\)</span></p></li>
<li><p>A <em>vector</em> is a 1D ordered array of <em>n</em> scalars, denoted by a bold letter: <span class="math notranslate nohighlight">\(\mathbf{x}=[3.24, 1.2]\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>th element of a vector, thus <span class="math notranslate nohighlight">\(x_0 = 3.24\)</span>.</p>
<ul>
<li><p>Note: some other courses use <span class="math notranslate nohighlight">\(x^{(i)}\)</span> notation</p></li>
</ul>
</li>
</ul>
</li>
<li><p>A <em>set</em> is an <em>unordered</em> collection of unique elements, denote by caligraphic capital: <span class="math notranslate nohighlight">\(\mathcal{S}=\{3.24, 1.2\}\)</span></p></li>
<li><p>A <em>matrix</em> is a 2D array of scalars, denoted by bold capital: <span class="math notranslate nohighlight">\(\mathbf{X}=\begin{bmatrix}
3.24 &amp; 1.2 \\
2.24 &amp; 0.2 
\end{bmatrix}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textbf{X}_{i}\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>th <em>row</em> of the matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{X}_{:,j}\)</span> denotes the <span class="math notranslate nohighlight">\(j\)</span>th <em>column</em></p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{X}_{i,j}\)</span> denotes the <em>element</em> in the <span class="math notranslate nohighlight">\(i\)</span>th row, <span class="math notranslate nohighlight">\(j\)</span>th column, thus <span class="math notranslate nohighlight">\(\mathbf{X}_{1,0} = 2.24\)</span></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}^{n \times p}\)</span>, an <span class="math notranslate nohighlight">\(n \times p\)</span> matrix, can represent <span class="math notranslate nohighlight">\(n\)</span> data points in a <span class="math notranslate nohighlight">\(p\)</span>-dimensional space</p>
<ul>
<li><p>Every row is a vector that can represent a <em>point</em> in an n-dimensional space, given a <em>basis</em>.</p></li>
<li><p>The <em>standard basis</em> for a Euclidean space is the set of unit vectors</p></li>
</ul>
</li>
<li><p>E.g. if <span class="math notranslate nohighlight">\(\mathbf{X}=\begin{bmatrix}
3.24 &amp; 1.2 \\
2.24 &amp; 0.2 \\
3.0 &amp; 0.6 
\end{bmatrix}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">3.24</span> <span class="p">,</span> <span class="mf">1.2</span> <span class="p">],[</span><span class="mf">2.24</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],[</span><span class="mf">3.0</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">]])</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]);</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_4_0.png" src="../_images/02 - Linear Models_4_0.png" />
</div>
</div>
<ul class="simple">
<li><p>A <em>tensor</em> is an <em>k</em>-dimensional array of data, denoted by an italic capital: <span class="math notranslate nohighlight">\(T\)</span></p>
<ul>
<li><p><em>k</em> is also called the order, degree, or rank</p></li>
<li><p><span class="math notranslate nohighlight">\(T_{i,j,k,...}\)</span> denotes the element or sub-tensor in the corresponding position</p></li>
<li><p>A set of color images can be represented by:</p>
<ul>
<li><p>a 4D tensor (sample x height x width x color channel)</p></li>
<li><p>a 2D tensor (sample x flattened vector of pixel values)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_images.png" alt="ml" style="width: 40%;"/><div class="section" id="basic-operations">
<h3>Basic operations<a class="headerlink" href="#basic-operations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Sums and products are denoted by capital Sigma and capital Pi:
$<span class="math notranslate nohighlight">\(\sum_{i=0}^{p} = x_0 + x_1 + ... + x_p \quad \prod_{i=0}^{p} = x_0 \cdot x_1 \cdot ... \cdot x_p\)</span>$</p></li>
<li><p>Operations on vectors are element-wise: e.g. <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{z} = [x_0+z_0,x_1+z_1, ... , x_p+z_p]\)</span></p></li>
<li><p>Dot product <span class="math notranslate nohighlight">\(\mathbf{w}\mathbf{x} = \mathbf{w} \cdot \mathbf{x} = \mathbf{w}^{T} \mathbf{x} = \sum_{i=0}^{p} w_i \cdot x_i = w_0 \cdot x_0 + w_1 \cdot x_1 + ... + w_p \cdot x_p\)</span></p></li>
<li><p>Matrix product <span class="math notranslate nohighlight">\(\mathbf{W}\mathbf{x} = \begin{bmatrix}
\mathbf{w_0} \cdot \mathbf{x} \\
... \\
\mathbf{w_p} \cdot \mathbf{x} \end{bmatrix}\)</span></p></li>
<li><p>A function <span class="math notranslate nohighlight">\(f(x) = y\)</span> relates an input element <span class="math notranslate nohighlight">\(x\)</span> to an output <span class="math notranslate nohighlight">\(y\)</span></p>
<ul>
<li><p>It has a <em>local minimum</em> at <span class="math notranslate nohighlight">\(x=c\)</span> if <span class="math notranslate nohighlight">\(f(x) \geq f(c)\)</span> in interval <span class="math notranslate nohighlight">\((c-\epsilon, c+\epsilon)\)</span></p></li>
<li><p>It has a <em>global minimum</em> at <span class="math notranslate nohighlight">\(x=c\)</span> if <span class="math notranslate nohighlight">\(f(x) \geq f(c)\)</span> for any value for <span class="math notranslate nohighlight">\(x\)</span></p></li>
</ul>
</li>
<li><p>A vector function consumes an input and produces a vector: <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = \mathbf{y}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\underset{x\in X}{\operatorname{max}}f(x)\)</span> returns the highest value f(x) for any x</p></li>
<li><p><span class="math notranslate nohighlight">\(\underset{x\in X}{\operatorname{argmax}}f(x)\)</span> returns the element x that maximizes f(x)</p></li>
</ul>
</div>
<div class="section" id="gradients">
<h3>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A <em>derivative</em> <span class="math notranslate nohighlight">\(f'\)</span> of a function <span class="math notranslate nohighlight">\(f\)</span> describes how fast <span class="math notranslate nohighlight">\(f\)</span> grows or decreases</p></li>
<li><p>The process of finding a derivative is called differentiation</p>
<ul>
<li><p>Derivatives for basic functions are known</p></li>
<li><p>For non-basic functions we use the chain rule: <span class="math notranslate nohighlight">\(F(x) = f(g(x)) \rightarrow F'(x)=f'(g(x))g'(x)\)</span></p></li>
</ul>
</li>
<li><p>A function is <em>differentiable</em> if it has a derivate in any point of it’s domain</p>
<ul>
<li><p>It’s <em>continuously differentiable</em> if <span class="math notranslate nohighlight">\(f'\)</span> is itself a function</p></li>
<li><p>It’s <em>smooth</em> if <span class="math notranslate nohighlight">\(f', f'', f''', ...\)</span> all exist</p></li>
</ul>
</li>
<li><p>A <em>gradient</em> <span class="math notranslate nohighlight">\(\nabla f\)</span> is the derivate of a function in multiple dimensions</p>
<ul>
<li><p>It is a vector of partial derivatives: <span class="math notranslate nohighlight">\(\nabla f = \left[ \frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1},... \right]\)</span></p></li>
<li><p>E.g. <span class="math notranslate nohighlight">\(f=2x_0+3x_1^{2}-\sin(x_2) \rightarrow \nabla f= [2, 6x_1, -cos(x_2)]\)</span></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Example: <span class="math notranslate nohighlight">\(f = -(x_0^2+x_1^2)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\nabla f = \left[\frac{\partial f}{\partial x_0},\frac{\partial f}{\partial x_1}\right] = \left[-2x_0,-2x_1\right]\)</span></p></li>
<li><p>Evaluated at point (-4,1): <span class="math notranslate nohighlight">\(\nabla f(-4,1) = [8,-2]\)</span></p>
<ul>
<li><p>These are the slopes at point (-4,1) in the direction of <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span> respectively</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interact_manual</span>

<span class="c1"># f = -(x0^2 + x1^2)</span>
<span class="k">def</span> <span class="nf">g_f</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">x0</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">g_dfx0</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x0</span>
<span class="k">def</span> <span class="nf">g_dfx1</span><span class="p">(</span><span class="n">x1</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x1</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_gradient</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">240</span><span class="p">,</span><span class="mi">10</span><span class="p">)):</span>
    <span class="c1"># plot surface of f</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">X0</span><span class="p">,</span> <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">g_f</span><span class="p">(</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">),</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;winter&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># choose point to evaluate: (-4,1)</span>
    <span class="n">i0</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
    <span class="n">i1</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">g_f</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span><span class="n">i1</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;($i_0$,$i_1$) = (-4,1)&#39;</span><span class="p">)</span>

    <span class="c1"># plot intersects</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">x0</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">30</span><span class="p">,</span><span class="n">g_f</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;$f(x_0,i_1)$&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">30</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">g_f</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;$f(i_0,x_1)$&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

    <span class="c1"># df/dx0 is slope of line at the intersect point</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">x0</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">30</span><span class="p">,</span><span class="n">g_dfx0</span><span class="p">(</span><span class="n">i0</span><span class="p">)</span><span class="o">*</span><span class="n">x0</span><span class="o">-</span><span class="n">g_f</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span><span class="n">i1</span><span class="p">),</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\frac{\partial f}{\partial x_0}(i_0,i_1) x_0 + f(i_0,i_1)$&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">30</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span><span class="n">g_dfx1</span><span class="p">(</span><span class="n">i1</span><span class="p">)</span><span class="o">*</span><span class="n">x1</span><span class="o">+</span><span class="n">g_f</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span><span class="n">i1</span><span class="p">),</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\frac{\partial f}{\partial x_1}(i_0,i_1) x_1 + f(i_0,i_1)$&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x0&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_zaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">rotation</span><span class="p">)</span> <span class="c1"># Use this to rotate the figure</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "5c0c71e94a3d4722b6dcc78ff434331e", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_gradient</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="distributions-and-probabilities">
<h3>Distributions and Probabilities<a class="headerlink" href="#distributions-and-probabilities" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The normal (Gaussian) distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is noted as <span class="math notranslate nohighlight">\(N(\mu,\sigma)\)</span></p></li>
<li><p>A random variable <span class="math notranslate nohighlight">\(X\)</span> can be continuous or discrete</p></li>
<li><p>A probability distribution <span class="math notranslate nohighlight">\(f_X\)</span> of a continuous variable <span class="math notranslate nohighlight">\(X\)</span>: <em>probability density function</em> (pdf)</p>
<ul>
<li><p>The <em>expectation</em> is given by <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \int x f_{X}(x) dx\)</span></p></li>
</ul>
</li>
<li><p>A probability distribution of a discrete variable: <em>probability mass function</em> (pmf)</p>
<ul>
<li><p>The <em>expectation</em> (or mean) <span class="math notranslate nohighlight">\(\mu_X = \mathbb{E}[X] = \sum_{i=1}^k[x_i \cdot Pr(X=x_i)]\)</span></p></li>
</ul>
</li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_pdf.png" alt="ml" style="width: 50%;"/></div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="linear-models">
<h1>Linear models<a class="headerlink" href="#linear-models" title="Permalink to this headline">¶</a></h1>
<p>Linear models make a prediction using a linear function of the input features <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w}}(\mathbf{x}) = \sum_{i=1}^{p} w_i \cdot x_i + w_{0}\]</div>
<p>Learn <span class="math notranslate nohighlight">\(w\)</span> from <span class="math notranslate nohighlight">\(X\)</span>, given a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>:
$<span class="math notranslate nohighlight">\(\underset{\mathbf{w}}{\operatorname{argmin}} \mathcal{L}(f_\mathbf{w}(X))\)</span>$</p>
<ul class="simple">
<li><p>Many algorithms with different <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>: Least squares, Ridge, Lasso, Logistic Regression, Linear SVMs,…</p></li>
<li><p>Can be very powerful (and fast), especially for large datasets with many features.</p></li>
<li><p>Can be generalized to learn non-linear patterns: <em>Generalized Linear Models</em></p>
<ul>
<li><p>Features can be augmentented with polynomials of the original features</p></li>
<li><p>Features can be transformed according to a distribution (Poisson, Tweedie, Gamma,…)</p></li>
<li><p>Some linear models (e.g. SVMs) can be <em>kernelized</em> to learn non-linear functions</p></li>
</ul>
</li>
</ul>
<div class="section" id="linear-models-for-regression">
<h2>Linear models for regression<a class="headerlink" href="#linear-models-for-regression" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Prediction formula for input features x:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(w_1\)</span> … <span class="math notranslate nohighlight">\(w_p\)</span> usually called <em>weights</em> or <em>coefficients</em> , <span class="math notranslate nohighlight">\(w_0\)</span> the <em>bias</em> or <em>intercept</em></p></li>
<li><p>Assumes that errors are <span class="math notranslate nohighlight">\(N(0,\sigma)\)</span></p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\hat{y} = \mathbf{w}\mathbf{x} + w_0 = \sum_{i=1}^{p} w_i \cdot x_i + w_0 = w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_p \cdot x_p + w_0 \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">mglearn.datasets</span> <span class="kn">import</span> <span class="n">make_wave</span>

<span class="n">Xw</span><span class="p">,</span> <span class="n">yw</span> <span class="o">=</span> <span class="n">make_wave</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">Xw_train</span><span class="p">,</span> <span class="n">Xw_test</span><span class="p">,</span> <span class="n">yw_train</span><span class="p">,</span> <span class="n">yw_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Xw</span><span class="p">,</span> <span class="n">yw</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xw_train</span><span class="p">,</span> <span class="n">yw_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w_1: </span><span class="si">%f</span><span class="s2">  w_0: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">line</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xw_train</span><span class="p">,</span> <span class="n">yw_train</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="c1">#plt.plot(X_test, y_test, &#39;.&#39;, c=&#39;r&#39;)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="s2">&quot;training data&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w_1: 0.393906  w_0: -0.031804
</pre></div>
</div>
<img alt="../_images/02 - Linear Models_14_1.png" src="../_images/02 - Linear Models_14_1.png" />
</div>
</div>
<div class="section" id="linear-regression-aka-ordinary-least-squares">
<h3>Linear Regression (aka Ordinary Least Squares)<a class="headerlink" href="#linear-regression-aka-ordinary-least-squares" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Loss function is the <em>sum of squared errors</em> (SSE) (or residuals) between predictions <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> (red) and the true regression targets <span class="math notranslate nohighlight">\(y_i\)</span> (blue) on the training set.
$<span class="math notranslate nohighlight">\(\mathcal{L}_{SSE} = \sum_{n=1}^{N} (y_n-\hat{y}_n)^2 = \sum_{n=1}^{N} (y_n-(\mathbf{w}\mathbf{x_n} + w_0))^2\)</span>$</p></li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_least_squares.png" alt="ml" style="margin: 0 auto; width: 550px;"/><div class="section" id="solving-ordinary-least-squares">
<h4>Solving ordinary least squares<a class="headerlink" href="#solving-ordinary-least-squares" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Convex optimization problem with unique closed-form solution:
$<span class="math notranslate nohighlight">\(w^{*} = (X^{T}X)^{-1} X^T Y\)</span>$</p>
<ul>
<li><p>Add a column of 1’s to the front of X to get <span class="math notranslate nohighlight">\(w_0\)</span></p></li>
<li><p>Slow. Time complexity is quadratic in number of features: <span class="math notranslate nohighlight">\(\mathcal{O}(p^2n)\)</span></p>
<ul>
<li><p>X has <span class="math notranslate nohighlight">\(n\)</span> rows, <span class="math notranslate nohighlight">\(p\)</span> features, hence <span class="math notranslate nohighlight">\(X^{T}X\)</span> has dimensionality <span class="math notranslate nohighlight">\(p x p\)</span></p></li>
</ul>
</li>
<li><p>Only works if <span class="math notranslate nohighlight">\(n&gt;p\)</span></p></li>
</ul>
</li>
<li><p><em>Gradient Descent</em></p>
<ul>
<li><p>Faster for large and/or high-dimensional datasets</p></li>
<li><p>When <span class="math notranslate nohighlight">\(X^{T}X\)</span> cannot be computed or takes too long (<span class="math notranslate nohighlight">\(p\)</span> or <span class="math notranslate nohighlight">\(n\)</span> is too large)</p></li>
</ul>
</li>
<li><p><strong>Very easily overfits</strong>.</p>
<ul>
<li><p>coefficients <span class="math notranslate nohighlight">\(w\)</span> become very large (steep incline/decline)</p></li>
<li><p>small change in the input <em>x</em> results in a very different output <em>y</em></p></li>
<li><p>No hyperparameters that control model complexity</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="gradient-descent">
<h4>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Start with an initial, random set of weights: <span class="math notranslate nohighlight">\(\mathbf{w}^0\)</span></p></li>
<li><p>Given a differentiable loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> (e.g. <span class="math notranslate nohighlight">\(\mathcal{L}_{SSE}\)</span>), compute <span class="math notranslate nohighlight">\(\nabla \mathcal{L}\)</span></p></li>
<li><p>For least squares: <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}_{SSE}}{\partial w_i}(\mathbf{w}) = -2 \sum_{n=1}^{N} (y_n-\hat{y}_n) x_{n,i}\)</span></p>
<ul>
<li><p>If feature <span class="math notranslate nohighlight">\(X_{:,i}\)</span> is associated with big errors, the gradient wrt <span class="math notranslate nohighlight">\(w_i\)</span> will be large</p></li>
</ul>
</li>
<li><p>Update <em>all</em> weights slightly (by <em>step size</em> or <em>learning rate</em> <span class="math notranslate nohighlight">\(\eta\)</span>) in ‘downhill’ direction.</p></li>
<li><p>Basic <em>update rule</em> (step s): $<span class="math notranslate nohighlight">\(\mathbf{w}^{s+1} = \mathbf{w}^s-\eta\nabla \mathcal{L}(\mathbf{w}^s)\)</span>$</p></li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent.jpg" alt="ml" style="width: 500px;"/><ul class="simple">
<li><p>Important hyperparameters</p>
<ul>
<li><p>Learning rate</p>
<ul>
<li><p>Too small: slow convergence. Too large: possible divergence</p></li>
</ul>
</li>
<li><p>Maximum number of iterations</p>
<ul>
<li><p>Too small: no convergence. Too large: wastes resources</p></li>
</ul>
</li>
<li><p>Learning rate decay with decay rate <span class="math notranslate nohighlight">\(k\)</span></p>
<ul>
<li><p>E.g. exponential (<span class="math notranslate nohighlight">\(\eta^{s+1} = \eta^{s}  e^{-ks}\)</span>), inverse-time (<span class="math notranslate nohighlight">\(\eta^{s+1} = \frac{\eta^{0}}{1+ks}\)</span>),…</p></li>
</ul>
</li>
<li><p>Many more advanced ways to control learning rate (see later)</p>
<ul>
<li><p>Adaptive techniques: depend on how much loss improved in previous step</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="c1"># Some convex function to represent the loss</span>
<span class="k">def</span> <span class="nf">l_fx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> 
<span class="c1"># Derivative to compute the gradient</span>
<span class="k">def</span> <span class="nf">l_dfx0</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">x0</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_learning_rate</span><span class="p">(</span><span class="n">learn_rate</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">exp_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">101</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">[</span><span class="n">l_fx</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">w_current</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.75</span>
    <span class="n">learn_rate_current</span> <span class="o">=</span> <span class="n">learn_rate</span>
    <span class="n">fw</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># weight values</span>
    <span class="n">fl</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># loss values</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">fw</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span>
        <span class="n">fl</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l_fx</span><span class="p">(</span><span class="n">w_current</span><span class="p">))</span>
        <span class="c1"># Decay</span>
        <span class="k">if</span> <span class="n">exp_decay</span><span class="p">:</span>
            <span class="n">learn_rate_current</span> <span class="o">=</span> <span class="n">learn_rate</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="o">*</span><span class="n">i</span><span class="p">)</span>
        <span class="c1"># Update rule</span>
        <span class="n">w_current</span> <span class="o">=</span> <span class="n">w_current</span> <span class="o">-</span> <span class="n">learn_rate_current</span> <span class="o">*</span> <span class="n">l_dfx0</span><span class="p">(</span><span class="n">w_current</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fw</span><span class="p">,</span> <span class="n">fl</span><span class="p">,</span> <span class="s1">&#39;--bo&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4394e4621dbe4108b71f471dcea5c7b1", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_learning_rate</span><span class="p">(</span><span class="n">learn_rate</span><span class="o">=</span><span class="mf">0.21</span><span class="p">,</span> <span class="n">exp_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In two dimensions:
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent_2D.png" alt="ml" style="width: 700px;"/></p>
<ul class="simple">
<li><p>You can get stuck in local minima (if the loss is not fully convex)</p>
<ul>
<li><p>If you have many model parameters, this is less likely</p></li>
<li><p>You always find a way down in some direction</p></li>
<li><p>Models with many parameters typically find good local minima</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Intuition: walking downhill using only the slope you “feel” nearby</p></li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent_hill.png" alt="ml" style="width: 800px;"/>
<p>(Image by A. Karpathy)</p>
</div>
<div class="section" id="stochastic-gradient-descent-sgd">
<h4>Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Compute gradients not on the entire dataset, but on a single data point <span class="math notranslate nohighlight">\(i\)</span> at a time</p>
<ul>
<li><p>Gradient descent: <span class="math notranslate nohighlight">\(\mathbf{w}^{s+1} = \mathbf{w}^s-\eta\nabla \mathcal{L}(\mathbf{w}^s) = \mathbf{w}^s-\frac{\eta}{n} \sum_{i=0}^{n} \nabla \mathcal{L_i}(\mathbf{w}^s)\)</span></p></li>
<li><p>Stochastic Gradient Descent: <span class="math notranslate nohighlight">\(\mathbf{w}^{s+1} = \mathbf{w}^s-\eta\nabla \mathcal{L_i}(\mathbf{w}^s)\)</span></p></li>
</ul>
</li>
<li><p>Many smoother variants, e.g.</p>
<ul>
<li><p>Minibatch SGD: compute gradient on batches of data: <span class="math notranslate nohighlight">\(\mathbf{w}^{s+1} = \mathbf{w}^s-\frac{\eta}{B} \sum_{i=0}^{B} \nabla \mathcal{L_i}(\mathbf{w}^s)\)</span></p></li>
<li><p>Stochastic Average Gradient Descent (SAG, SAGA)</p>
<ul>
<li><p>Incremental gradient: <span class="math notranslate nohighlight">\(\mathbf{w}^{s+1} = \mathbf{w}^s-\frac{\eta}{n} \sum_{i=0}^{n} v_i^s\)</span> with <span class="math notranslate nohighlight">\(v_i^s = \begin{cases}\nabla \mathcal{L_i}(\mathbf{w}^s) &amp; \text{random i} \\ v_i^{s-1} &amp; \text{otherwise} \end{cases}\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_SGD.png" alt="ml" style="float: left; width: 400px;"/></div>
<div class="section" id="in-practice">
<h4>In practice<a class="headerlink" href="#in-practice" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Linear regression can be found in <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code>. We’ll evaluate it on the Boston Housing dataset.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> uses closed form solution, <code class="docutils literal notranslate"><span class="pre">SGDRegressor</span></code> with <code class="docutils literal notranslate"><span class="pre">loss='squared_loss'</span></code> uses Stochastic Gradient Descent</p></li>
<li><p>Large coefficients signal overfitting</p></li>
<li><p>Test score is much lower than training score</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">X_B</span><span class="p">,</span> <span class="n">y_B</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_extended_boston</span><span class="p">()</span>
<span class="n">X_B_train</span><span class="p">,</span> <span class="n">X_B_test</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">,</span> <span class="n">y_B_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_B</span><span class="p">,</span> <span class="n">y_B</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights (coefficients): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">40</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bias (intercept): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights (coefficients): [ -412.711   -52.243  -131.899   -12.004   -15.511    28.716    54.704
   -49.535    26.582    37.062   -11.828   -18.058   -19.525    12.203
  2980.781  1500.843   114.187   -16.97     40.961   -24.264    57.616
  1278.121 -2239.869   222.825    -2.182    42.996   -13.398   -19.389
    -2.575   -81.013     9.66      4.914    -0.812    -7.647    33.784
   -11.446    68.508   -17.375    42.813     1.14 ]
Bias (intercept): 30.93456367364078
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score (R^2): </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score (R^2): </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_test</span><span class="p">,</span> <span class="n">y_B_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score (R^2): 0.95
Test set score (R^2): 0.61
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="ridge-regression">
<h3>Ridge regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Adds a penalty term to the least squares loss function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{Ridge} = \sum_{n=1}^{N} (y_n-(\mathbf{w}\mathbf{x_n} + w_0))^2 + \alpha \sum_{i=0}^{p} w_i^2\]</div>
<ul class="simple">
<li><p>Model is penalized if it uses large coefficients (<span class="math notranslate nohighlight">\(w\)</span>)</p>
<ul>
<li><p>Each feature should have as little effect on the outcome as possible</p></li>
</ul>
</li>
<li><p>Regularization: explicitly restrict a model to avoid overfitting.</p>
<ul>
<li><p>Called L2 regularization because it uses the L2 norm: <span class="math notranslate nohighlight">\(\sum w_i^2\)</span></p></li>
</ul>
</li>
<li><p>The strength of the regularization can be controlled with the <span class="math notranslate nohighlight">\(\alpha\)</span> hyperparameter.</p>
<ul>
<li><p>Increasing <span class="math notranslate nohighlight">\(\alpha\)</span> causes more regularization (or shrinkage). Default is 1.0.</p></li>
</ul>
</li>
<li><p>Still convex. Can be optimized in different ways:</p>
<ul>
<li><p>Closed form solution (a.k.a. Cholesky): <span class="math notranslate nohighlight">\(w^{*} = (X^{T}X + \alpha I)^{-1} X^T Y\)</span></p></li>
<li><p>Gradient descent and variants, e.g. Stochastic Average Gradient (SAG,SAGA)</p>
<ul>
<li><p>Conjugate gradient (CG): each new gradient is influenced by previous ones</p></li>
</ul>
</li>
<li><p>Use Cholesky for smaller datasets, Gradient descent for larger ones</p></li>
</ul>
</li>
</ul>
<div class="section" id="id1">
<h4>In practice<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights (coefficients): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">40</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bias (intercept): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_test</span><span class="p">,</span> <span class="n">y_B_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights (coefficients): [-1.414 -1.557 -1.465 -0.127 -0.079  8.332  0.255 -4.941  3.899 -1.059
 -1.584  1.051 -4.012  0.334  0.004 -0.849  0.745 -1.431 -1.63  -1.405
 -0.045 -1.746 -1.467 -1.332 -1.692 -0.506  2.622 -2.092  0.195 -0.275
  5.113 -1.671 -0.098  0.634 -0.61   0.04  -1.277 -2.913  3.395  0.792]
Bias (intercept): 21.39052595861006
Training set score: 0.89
Test set score: 0.75
</pre></div>
</div>
</div>
</div>
<p>Test set score is higher and training set score lower: less overfitting!</p>
<ul class="simple">
<li><p>We can plot the weight values for differents levels of regularization to explore the effect of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
<li><p>Increasing regularization decreases the values of the coefficients, but never to 0.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">0.05</span><span class="p">)):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mf">1.5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;alpha </span><span class="si">{}</span><span class="s2">, score </span><span class="si">{:.2f}</span><span class="s2"> (training score </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_test</span><span class="p">,</span> <span class="n">y_B_test</span><span class="p">),</span> <span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4e596f3fc21942bc95bbe628ba4f19ac", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
        <span class="n">plot_ridge</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When we plot the train and test scores for every <span class="math notranslate nohighlight">\(\alpha\)</span> value, we see a sweet spot around <span class="math notranslate nohighlight">\(\alpha=0.2\)</span></p>
<ul>
<li><p>Models with smaller <span class="math notranslate nohighlight">\(\alpha\)</span> are overfitting</p></li>
<li><p>Models with larger <span class="math notranslate nohighlight">\(\alpha\)</span> are underfitting</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ai</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alpha</span><span class="p">)))</span>
<span class="n">test_score</span><span class="o">=</span><span class="p">[]</span>
<span class="n">train_score</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alpha</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)</span>
    <span class="n">test_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_test</span><span class="p">,</span> <span class="n">y_B_test</span><span class="p">))</span>
    <span class="n">train_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_score</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test score&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_score</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train score&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_36_0.png" src="../_images/02 - Linear Models_36_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="other-ways-to-reduce-overfitting">
<h3>Other ways to reduce overfitting<a class="headerlink" href="#other-ways-to-reduce-overfitting" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add more training data: with enough training data, regularization becomes less important</p>
<ul>
<li><p>Ridge and ordinary least squares will have the same performance</p></li>
</ul>
</li>
<li><p>Use fewer features: remove unimportant ones or find a low-dimensional embedding (e.g. PCA)</p>
<ul>
<li><p>Fewer coefficients to learn, reduces the flexibility of the model</p></li>
</ul>
</li>
<li><p>Scaling the data typically helps (and changes the optimal <span class="math notranslate nohighlight">\(\alpha\)</span> value)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_ridge_n_samples</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_38_0.png" src="../_images/02 - Linear Models_38_0.png" />
</div>
</div>
</div>
<div class="section" id="lasso-least-absolute-shrinkage-and-selection-operator">
<h3>Lasso (Least Absolute Shrinkage and Selection Operator)<a class="headerlink" href="#lasso-least-absolute-shrinkage-and-selection-operator" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Adds a different penalty term to the least squares sum:
$<span class="math notranslate nohighlight">\(\mathcal{L}_{Lasso} = \sum_{n=1}^{N} (y_n-(\mathbf{w}\mathbf{x_n} + w_0))^2 + \alpha \sum_{i=0}^{p} |w_i|\)</span>$</p></li>
<li><p>Called L1 regularization because it uses the L1 norm</p>
<ul>
<li><p>Will cause many weights to be exactly 0</p></li>
</ul>
</li>
<li><p>Same parameter <span class="math notranslate nohighlight">\(\alpha\)</span> to control the strength of regularization.</p>
<ul>
<li><p>Will again have a ‘sweet spot’ depending on the data</p></li>
</ul>
</li>
<li><p>No closed-form solution</p></li>
<li><p>Convex, but no longer strictly convex, and not differentiable</p>
<ul>
<li><p>Weights can be optimized using <em>coordinate descent</em></p></li>
</ul>
</li>
</ul>
<p>Analyze what happens to the weights:</p>
<ul class="simple">
<li><p>L1 prefers coefficients to be exactly zero (sparse models)</p></li>
<li><p>Some features are ignored entirely: automatic feature selection</p></li>
<li><p>How can we explain this?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.005</span><span class="p">)):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mf">1.5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;alpha </span><span class="si">{}</span><span class="s2">, score </span><span class="si">{:.2f}</span><span class="s2"> (training score </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_test</span><span class="p">,</span> <span class="n">y_B_test</span><span class="p">),</span> <span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_B_train</span><span class="p">,</span> <span class="n">y_B_train</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "06c4e5bd6aa64f28bb822820f3381442", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]:</span>
        <span class="n">plot_lasso</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="coordinate-descent">
<h4>Coordinate descent<a class="headerlink" href="#coordinate-descent" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Alternative for gradient descent, supports non-differentiable convex loss functions (e.g. <span class="math notranslate nohighlight">\(\mathcal{L}_{Lasso}\)</span>)</p></li>
<li><p>In every iteration, optimize a single coordinate <span class="math notranslate nohighlight">\(w_i\)</span> (find minimum in direction of <span class="math notranslate nohighlight">\(x_i\)</span>)</p>
<ul>
<li><p>Continue with another coordinate, using a selection rule (e.g. round robin)</p></li>
</ul>
</li>
<li><p>Faster iterations. No need to choose a step size (learning rate).</p></li>
<li><p>May converge more slowly. Can’t be parallellized.</p></li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_cd.png" alt="ml" style="width: 400px;"/></div>
<div class="section" id="coordinate-descent-with-lasso">
<h4>Coordinate descent with Lasso<a class="headerlink" href="#coordinate-descent-with-lasso" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Remember that <span class="math notranslate nohighlight">\(\mathcal{L}_{Lasso} = \mathcal{L}_{SSE} + \alpha \sum_{i=1}^{p} |w_i|\)</span></p></li>
<li><p>For one <span class="math notranslate nohighlight">\(w_i\)</span>: <span class="math notranslate nohighlight">\(\mathcal{L}_{Lasso}(w_i) = \mathcal{L}_{SSE}(w_i) + \alpha |w_i|\)</span></p></li>
<li><p>The L1 term is not differentiable but convex: we can compute the <a class="reference external" href="https://towardsdatascience.com/unboxing-lasso-regularization-with-proximal-gradient-method-ista-iterative-soft-thresholding-b0797f05f8ea"><em>subgradient</em></a></p>
<ul>
<li><p>Unique at points where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is differentiable, a range of all possible slopes [a,b] where it is not</p></li>
<li><p>For <span class="math notranslate nohighlight">\(|w_i|\)</span>, the subgradient <span class="math notranslate nohighlight">\(\partial_{w_i} |w_i|\)</span> =  <span class="math notranslate nohighlight">\(\begin{cases}-1 &amp; w_i&lt;0\\ [-1,1] &amp; w_i=0 \\ 1 &amp; w_i&gt;0 \\ \end{cases}\)</span></p></li>
<li><p>Subdifferential <span class="math notranslate nohighlight">\(\partial(f+g) = \partial f + \partial g\)</span> if <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are both convex</p></li>
</ul>
</li>
<li><p>To find the optimum for Lasso <span class="math notranslate nohighlight">\(w_i^{*}\)</span>, solve
$<span class="math notranslate nohighlight">\(\begin{aligned} \partial_{w_i} \mathcal{L}_{Lasso}(w_i) &amp;= \partial_{w_i} \mathcal{L}_{SSE}(w_i) + \partial_{w_i} \alpha |w_i| \\ 0 &amp;= (w_i - \rho_i) + \alpha \cdot \partial_{w_i} |w_i| \\ w_i &amp;= \rho_i - \alpha \cdot \partial_{w_i} |w_i| \end{aligned}\)</span>$</p>
<ul>
<li><p>In which <span class="math notranslate nohighlight">\(\rho_i\)</span> is the solution for <span class="math notranslate nohighlight">\(\mathcal{L}_{SSE}(w_i)\)</span></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>We found: <span class="math notranslate nohighlight">\(w_i = \rho_i - \alpha \cdot \partial_{w_i} |w_i|\)</span></p></li>
<li><p>Lasso solution has the form of a <em>soft thresholding function</em> <span class="math notranslate nohighlight">\(S\)</span>
$<span class="math notranslate nohighlight">\(w_i^* = S(\rho_i,\alpha) = \begin{cases} \rho_i + \alpha, &amp; \rho_i &lt; -\alpha \\  0, &amp; -\alpha &lt; \rho_i &lt; \alpha \\ \rho_i - \alpha, &amp; \rho_i &gt; \alpha \\ \end{cases}\)</span>$</p>
<ul>
<li><p>Small weights become 0: sparseness!</p></li>
<li><p>If the data is not normalized, <span class="math notranslate nohighlight">\(w_i^* = \frac{1}{z_i}S(\rho_i,\alpha)\)</span> with <span class="math notranslate nohighlight">\(z_i\)</span> a normalizing constant</p></li>
</ul>
</li>
<li><p>Ridge solution: <span class="math notranslate nohighlight">\(w_i = \rho_i - \alpha \cdot \partial_{w_i} w_i^2 = \rho_i - 2\alpha \cdot w_i\)</span>, thus <span class="math notranslate nohighlight">\(w_i^* = \frac{\rho_i}{1 + 2\alpha}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_rho</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">0.05</span><span class="p">)):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">101</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">w</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">+</span><span class="n">alpha</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">alpha</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">alpha</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="n">alpha</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\rho$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$w^{*}$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ordinary Least Squares (SSE)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ridge with alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Lasso with alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "328c64490c0b4a7493056ddf2c82b9ce", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_rho</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="interpreting-l1-and-l2-loss">
<h3>Interpreting L1 and L2 loss<a class="headerlink" href="#interpreting-l1-and-l2-loss" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>L1 and L2 in function of the weights</p></li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/L12_1.png" alt="ml" style="width: 700px;"/><p>Least Squares Loss + L1 or L2</p>
<ul class="simple">
<li><p>Lasso is not differentiable at point 0</p></li>
<li><p>For any minimum of least squares, L2 will be smaller, and L1 is more likely be 0</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">c_fx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">fX</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># Some convex function to represent the loss</span>
    <span class="k">return</span> <span class="n">fX</span><span class="o">/</span><span class="mi">9</span> <span class="c1"># Scaling</span>
<span class="k">def</span> <span class="nf">c_fl2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">c_fx</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="k">def</span> <span class="nf">c_fl1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">c_fx</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">l2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="k">def</span> <span class="nf">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">alpha</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_losses</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.05</span><span class="p">)):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">101</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_fx</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_fl2</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_fl1</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">rp</span> <span class="o">=</span> <span class="p">[</span><span class="n">l2</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="p">[</span><span class="n">l1</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">rp</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;L2 with alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">lp</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;L1 with alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Least Squares loss&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Loss + L2 (Ridge)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Loss + L1 (Lasso)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">opt_f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">opt_f</span><span class="p">],</span> <span class="n">f</span><span class="p">[</span><span class="n">opt_f</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">opt_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">opt_r</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="n">opt_r</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">opt_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">opt_l</span><span class="p">],</span> <span class="n">l</span><span class="p">[</span><span class="n">opt_l</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "24c965aec44a43e0a605c34b8737edb8", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_losses</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>In 2D (for 2 model weights <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span>)</p>
<ul>
<li><p>The least squared loss is a 2D convex function in this space</p></li>
<li><p>For illustration, assume that L1 loss = L2 loss = 1</p>
<ul>
<li><p>L1 loss (<span class="math notranslate nohighlight">\(\Sigma |w_i|\)</span>): every {<span class="math notranslate nohighlight">\(w_1, w_2\)</span>} falls on the diamond</p></li>
<li><p>L2 loss (<span class="math notranslate nohighlight">\(\Sigma w_i^2\)</span>): every {<span class="math notranslate nohighlight">\(w_1, w_2\)</span>} falls on the circle</p></li>
</ul>
</li>
<li><p>For L1, the loss is minimized if <span class="math notranslate nohighlight">\(w_1\)</span> or <span class="math notranslate nohighlight">\(w_2\)</span> is 0 (rarely so for L2)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_loss_interpretation</span><span class="p">():</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>

    <span class="n">l2</span> <span class="o">=</span> <span class="n">xx</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">yy</span><span class="p">)</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="n">elastic_net</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">l1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">l2</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="n">elastic_net_contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">elastic_net</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;darkorange&quot;</span><span class="p">)</span>
    <span class="n">l2_contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;c&quot;</span><span class="p">)</span>
    <span class="n">l1_contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;navy&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;bottom&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">elastic_net_contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">fmt</span><span class="o">=</span><span class="p">{</span><span class="mf">1.0</span><span class="p">:</span> <span class="s1">&#39;elastic-net&#39;</span><span class="p">},</span> <span class="n">manual</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">)])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">l2_contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">fmt</span><span class="o">=</span><span class="p">{</span><span class="mf">1.0</span><span class="p">:</span> <span class="s1">&#39;L2&#39;</span><span class="p">},</span> <span class="n">manual</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">l1_contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">fmt</span><span class="o">=</span><span class="p">{</span><span class="mf">1.0</span><span class="p">:</span> <span class="s1">&#39;L1&#39;</span><span class="p">},</span> <span class="n">manual</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)])</span>

    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">X1</span><span class="o">/</span><span class="mi">2</span><span class="o">-</span><span class="mf">0.7</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">X2</span><span class="o">/</span><span class="mi">4</span><span class="o">-</span><span class="mf">0.28</span><span class="p">))</span>
    <span class="n">cp</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="c1">#plt.clabel(cp, inline=1, fontsize=10)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plot_loss_interpretation</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_53_0.png" src="../_images/02 - Linear Models_53_0.png" />
</div>
</div>
</div>
<div class="section" id="elastic-net">
<h3>Elastic-Net<a class="headerlink" href="#elastic-net" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Adds both L1 and L2 regularization:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{Elastic} = \sum_{n=1}^{N} (y_n-(\mathbf{w}\mathbf{x_n} + w_0))^2 + \alpha \rho \sum_{i=0}^{p} |w_i| + \alpha (1 -  \rho) \sum_{i=0}^{p} w_i^2\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho\)</span> is the L1 ratio</p>
<ul>
<li><p>With <span class="math notranslate nohighlight">\(\rho=1\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_{Elastic} = \mathcal{L}_{Lasso}\)</span></p></li>
<li><p>With <span class="math notranslate nohighlight">\(\rho=0\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}_{Elastic} = \mathcal{L}_{Ridge}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(0 &lt; \rho &lt; 1\)</span> sets a trade-off between L1 and L2.</p></li>
</ul>
</li>
<li><p>Allows learning sparse models (like Lasso) while maintaining L2 regularization benefits</p>
<ul>
<li><p>E.g. if 2 features are correlated, Lasso likely picks one randomly, Elastic-Net keeps both</p></li>
</ul>
</li>
<li><p>Weights can be optimized using coordinate descent (similar to Lasso)</p></li>
</ul>
</div>
<div class="section" id="other-loss-functions-for-regression">
<h3>Other loss functions for regression<a class="headerlink" href="#other-loss-functions-for-regression" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Huber loss: switches from squared loss to linear loss past a value <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<ul>
<li><p>More robust against outliers</p></li>
</ul>
</li>
<li><p>Epsilon insensitive: ignores errors smaller than <span class="math notranslate nohighlight">\(\epsilon\)</span>, and linear past that</p>
<ul>
<li><p>Aims to fit function so that residuals are at most <span class="math notranslate nohighlight">\(\epsilon\)</span></p></li>
<li><p>Also known as Support Vector Regression (<code class="docutils literal notranslate"><span class="pre">SVR</span></code> in sklearn)</p></li>
</ul>
</li>
<li><p>Squared Epsilon insensitive: ignores errors smaller than <span class="math notranslate nohighlight">\(\epsilon\)</span>, and squared past that</p></li>
<li><p>These can all be solved with stochastic gradient descent</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">SGDRegressor</span></code> in sklearn</p></li>
</ul>
</li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/huber.png" alt="ml" style="width: 350px;"/></div>
</div>
<div class="section" id="linear-models-for-classification">
<h2>Linear models for Classification<a class="headerlink" href="#linear-models-for-classification" title="Permalink to this headline">¶</a></h2>
<p>Aims to find a hyperplane that separates the examples of each class.<br />
For binary classification (2 classes), we aim to fit the following function:</p>
<p><span class="math notranslate nohighlight">\(\hat{y} = w_1 * x_1 + w_2 * x_2 +... + w_p * x_p + w_0 &gt; 0\)</span></p>
<p>When <span class="math notranslate nohighlight">\(\hat{y}&lt;0\)</span>, predict class -1, otherwise predict class +1</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>

<span class="n">Xf</span><span class="p">,</span> <span class="n">yf</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_forge</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xf</span><span class="p">,</span> <span class="n">yf</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">Xf</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.7</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cm</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">Xf</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xf</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">yf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="c1">#[&#39;Class -1&#39;,&#39;Class 1&#39;]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_57_0.png" src="../_images/02 - Linear Models_57_0.png" />
</div>
</div>
<ul class="simple">
<li><p>There are many algorithms for linear classification, differing in loss function, regularization techniques, and optimization method</p></li>
<li><p>Most common techniques:</p>
<ul>
<li><p>Convert target classes {neg,pos} to {0,1} and treat as a regression task</p>
<ul>
<li><p>Logistic regression (Log loss)</p></li>
<li><p>Ridge Classification (Least Squares + L2 loss)</p></li>
</ul>
</li>
<li><p>Find hyperplane that maximizes the margin between classes</p>
<ul>
<li><p>Linear Support Vector Machines (Hinge loss)</p></li>
</ul>
</li>
<li><p>Neural networks without activation functions</p>
<ul>
<li><p>Perceptron (Perceptron loss)</p></li>
</ul>
</li>
<li><p>SGDClassifier: can act like any of these by choosing loss function</p>
<ul>
<li><p>Hinge, Log, Modified_huber, Squared_hinge, Perceptron</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="section" id="logistic-regression">
<h3>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Aims to predict the <em>probability</em> that a point belongs to the positive class</p></li>
<li><p>Converts target values {negative (blue), positive (red)} to {0,1}</p></li>
<li><p>Fits a <em>logistic</em> (or <em>sigmoid</em> or <em>S</em> curve) function through these points</p>
<ul>
<li><p>Maps (-Inf,Inf) to a probability [0,1]
$<span class="math notranslate nohighlight">\( \hat{y} = \textrm{logistic}(f_{\theta}(\mathbf{x})) = \frac{1}{1+e^{-f_{\theta}(\mathbf{x})}} \)</span>$</p></li>
</ul>
</li>
<li><p>E.g. in 1D: <span class="math notranslate nohighlight">\( \textrm{logistic}(x_1w_1+w_0) = \frac{1}{1+e^{-x_1w_1-w_0}} \)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w0</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">w1</span><span class="o">+</span><span class="n">w0</span><span class="p">)))</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_logreg</span><span class="p">(</span><span class="n">w0</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">15.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">w1</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">red</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xf</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">yf</span><span class="p">))</span> <span class="k">if</span> <span class="n">yf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">blue</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xf</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">yf</span><span class="p">))</span> <span class="k">if</span> <span class="n">yf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">red</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">red</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">blue</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">blue</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">],</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">w0</span><span class="o">/</span><span class="n">w1</span><span class="p">),</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">w1</span><span class="o">+</span><span class="n">w0</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature2&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">1.05</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;logistic(x*w1+w0)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span><span class="s1">&#39;Decision boundary&#39;</span><span class="p">,</span><span class="s1">&#39;y=x*w1+w0&#39;</span><span class="p">,</span><span class="s1">&#39;Positive class&#39;</span><span class="p">,</span><span class="s1">&#39;Negative class&#39;</span><span class="p">],</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "9c6d3e55f76e461ab16fd79aeb4044b5", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="c1"># fitted solution</span>
    <span class="n">clf2</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xf</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">yf</span><span class="p">)</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="n">clf2</span><span class="o">.</span><span class="n">intercept_</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">clf2</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">plot_logreg</span><span class="p">(</span><span class="n">w0</span><span class="o">=</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="o">=</span><span class="n">w1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Fitted solution to our 2D example:</p>
<ul>
<li><p>To get a binary prediction, choose a probability threshold (e.g. 0.5)</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xf</span><span class="p">,</span> <span class="n">yf</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x2</span><span class="o">*</span><span class="n">w2</span><span class="o">+</span><span class="n">x1</span><span class="o">*</span><span class="n">w1</span><span class="o">+</span><span class="n">w0</span><span class="p">)))</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_logistic_fit</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">360</span><span class="p">,</span><span class="mi">10</span><span class="p">)):</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="n">lr_clf</span><span class="o">.</span><span class="n">intercept_</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">lr_clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">lr_clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># plot surface of f</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">X0</span><span class="p">,</span> <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    
    <span class="c1"># Surface</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">sigmoid2d</span><span class="p">(</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bwr&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="c1"># Points</span>
    <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">Xf</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xf</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">yf</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">yf</span><span class="p">])</span>
    
    <span class="c1"># Decision boundary</span>
    <span class="c1"># x2 = -(x1*w1 + w0)/w2</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">x0</span><span class="o">*</span><span class="n">w1</span> <span class="o">+</span> <span class="n">w0</span><span class="p">)</span><span class="o">/</span><span class="n">w2</span><span class="p">,[</span><span class="mf">0.5</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
    <span class="n">XZ</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">YZ</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">XZ</span><span class="o">*</span><span class="n">w1</span> <span class="o">+</span> <span class="n">w0</span><span class="p">)</span><span class="o">/</span><span class="n">w2</span>    
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">XZ</span><span class="p">,</span> <span class="n">YZ</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;decision boundary&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x0&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_zaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">rotation</span><span class="p">)</span> <span class="c1"># Use this to rotate the figure</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="c1">#plt.legend() # Doesn&#39;t work yet, bug in matplotlib</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "50b44f4c60f54d619ac1690622761349", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_logistic_fit</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">180</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="loss-function-cross-entropy">
<h4>Loss function: Cross-entropy<a class="headerlink" href="#loss-function-cross-entropy" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Models that return class probabilities can use <em>cross-entropy loss</em>
$<span class="math notranslate nohighlight">\(\mathcal{L_{log}}(\mathbf{w}) = \sum_{n=1}^{N} H(p_n,q_n) = - \sum_{n=1}^{N} \sum_{c=1}^{C} p_{n,c} log(q_{n,c}) \)</span>$</p>
<ul>
<li><p>Also known as log loss, logistic loss, or maximum likelihood</p></li>
<li><p>Based on true probabilities <span class="math notranslate nohighlight">\(p\)</span> (0 or 1) and predicted probabilities <span class="math notranslate nohighlight">\(q\)</span> over <span class="math notranslate nohighlight">\(N\)</span> instances and <span class="math notranslate nohighlight">\(C\)</span> classes</p>
<ul>
<li><p>Binary case (C=2): <span class="math notranslate nohighlight">\(\mathcal{L_{log}}(\mathbf{w}) = - \sum_{n=1}^{N} \big[ y_n log(\hat{y}_n) + (1-y_n) log(1-\hat{y}_n) \big]\)</span></p></li>
</ul>
</li>
<li><p>Penalty (or surprise) grows exponentially as difference between <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> increases</p></li>
<li><p>Often used together with L2 (or L1) loss: <span class="math notranslate nohighlight">\(\mathcal{L_{log}}'(\mathbf{w}) = \mathcal{L_{log}}(\mathbf{w}) + \alpha \sum_{i} w_i^2 \)</span></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">yHat</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yHat</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 1&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 0&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Predicted probability $\hat</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_66_0.png" src="../_images/02 - Linear Models_66_0.png" />
</div>
</div>
</div>
<div class="section" id="optimization-methods-solvers-for-cross-entropy-loss">
<h4>Optimization methods (solvers) for cross-entropy loss<a class="headerlink" href="#optimization-methods-solvers-for-cross-entropy-loss" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Gradient descent (only supports L2 regularization)</p>
<ul>
<li><p>Log loss is differentiable, so we can use (stochastic) gradient descent</p></li>
<li><p>Variants thereof, e.g. Stochastic Average Gradient (SAG, SAGA)</p></li>
</ul>
</li>
<li><p>Coordinate descent (supports both L1 and L2 regularization)</p>
<ul>
<li><p>Faster iteration, but may converge more slowly, has issues with saddlepoints</p></li>
<li><p>Called <code class="docutils literal notranslate"><span class="pre">liblinear</span></code> in sklearn. Can’t run in parallel.</p></li>
</ul>
</li>
<li><p>Newton-Rhapson or Newton Conjugate Gradient (only L2):</p>
<ul>
<li><p>Uses the Hessian <span class="math notranslate nohighlight">\(H = \big[\frac{\partial^2 \mathcal{L}}{\partial x_i \partial x_j} \big]\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}^{s+1} = \mathbf{w}^s-\eta H^{-1}(\mathbf{w}^s) \nabla \mathcal{L}(\mathbf{w}^s)\)</span></p></li>
<li><p>Slow for large datasets. Works well if solution space is (near) convex</p></li>
</ul>
</li>
<li><p>Quasi-Newton methods (only L2)</p>
<ul>
<li><p>Approximate, faster to compute</p></li>
<li><p>E.g. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (<code class="docutils literal notranslate"><span class="pre">lbfgs</span></code>)</p>
<ul>
<li><p>Default in sklearn for Logistic Regression</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451">Some hints on choosing solvers</a></p>
<ul>
<li><p>Data scaling helps convergence, minimizes differences between solvers</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id2">
<h4>In practice<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Logistic regression can also be found in <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter is the <em>inverse</em> regularization strength: <span class="math notranslate nohighlight">\(C=\alpha^{-1}\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">penalty</span></code>: type of regularization: L1, L2 (default), Elastic-Net, or None</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">solver</span></code>: newton-cg, lbfgs (default), liblinear, sag, saga</p></li>
</ul>
</li>
<li><p>Increasing C: less regularization, tries to overfit individual points</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_lr</span><span class="p">(</span><span class="n">C_log</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)):</span>
    <span class="c1"># Still using artificial data</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">Xf</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xf</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">yf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="n">C_log</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xf</span><span class="p">,</span> <span class="n">yf</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
    <span class="n">yy</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">-</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;C = </span><span class="si">{:.3f}</span><span class="s2">, w1=</span><span class="si">{:.3f}</span><span class="s2">, w2=</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">C_log</span><span class="p">,</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "28e9ae1cf67f4da5b6f8c31d83314ce5", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_lr</span><span class="p">(</span><span class="n">C_log</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Analyze behavior on the breast cancer dataset</p>
<ul>
<li><p>Underfitting if C is too small, some overfitting if C is too large</p></li>
<li><p>We use cross-validation because the dataset is small</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X_C</span><span class="p">,</span> <span class="n">y_C</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span>

<span class="n">C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">test_score</span><span class="o">=</span><span class="p">[]</span>
<span class="n">train_score</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">C</span><span class="p">:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span><span class="n">X_C</span><span class="p">,</span><span class="n">y_C</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">)</span>
    <span class="n">test_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_score&#39;</span><span class="p">]))</span>
    <span class="n">train_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;train_score&#39;</span><span class="p">]))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_score</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test score&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_score</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train score&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_72_0.png" src="../_images/02 - Linear Models_72_0.png" />
</div>
</div>
<ul class="simple">
<li><p>Again, choose between L1 or L2 regularization (or elastic-net)</p></li>
<li><p>Small C overfits, L1 leads to sparse models</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_C_train</span><span class="p">,</span> <span class="n">X_C_test</span><span class="p">,</span> <span class="n">y_C_train</span><span class="p">,</span> <span class="n">y_C_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_C</span><span class="p">,</span> <span class="n">y_C</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_logreg</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">100.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">penalty</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span><span class="s1">&#39;l2&#39;</span><span class="p">]):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_C_train</span><span class="p">,</span> <span class="n">y_C_train</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mf">1.7</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;C: </span><span class="si">{:.3f}</span><span class="s2">, penalty: </span><span class="si">{}</span><span class="s2">, score </span><span class="si">{:.2f}</span><span class="s2"> (training score </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_C_test</span><span class="p">,</span> <span class="n">y_C_test</span><span class="p">),</span> <span class="n">r</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_C_train</span><span class="p">,</span> <span class="n">y_C_train</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Coeff. magnitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "8db046ad23c247d9b3f12b69867005de", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_logreg</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">)</span>
    <span class="n">plot_logreg</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">)</span>
    <span class="n">plot_logreg</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="ridge-classification">
<h3>Ridge Classification<a class="headerlink" href="#ridge-classification" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Instead of log loss, we can also use ridge loss:
$<span class="math notranslate nohighlight">\(\mathcal{L}_{Ridge} = \sum_{n=1}^{N} (y_n-(\mathbf{w}\mathbf{x_n} + w_0))^2 + \alpha \sum_{i=0}^{p} w_i^2\)</span>$</p></li>
<li><p>In this case, target values {negative, positive} are converted to {-1,1}</p></li>
<li><p>Can be solved similarly to Ridge regression:</p>
<ul>
<li><p>Closed form solution (a.k.a. Cholesky)</p></li>
<li><p>Gradient descent and variants</p>
<ul>
<li><p>E.g. Conjugate Gradient (CG) or Stochastic Average Gradient (SAG,SAGA)</p></li>
</ul>
</li>
<li><p>Use Cholesky for smaller datasets, Gradient descent for larger ones</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="support-vector-machines">
<h3>Support vector machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Decision boundaries close to training points may generalize badly</p>
<ul>
<li><p>Very similar (nearby) test point are classified as the other class</p></li>
</ul>
</li>
<li><p>Choose a boundary that is as far away from training points as possible</p></li>
<li><p>The <strong>support vectors</strong> are the training samples closest to the hyperplane</p></li>
<li><p>The <strong>margin</strong> is the distance between the separating hyperplane and the <em>support vectors</em></p></li>
<li><p>Hence, our objective is to <em>maximize the margin</em>
<img src="../images/05_margin.png" alt="ml" style="width: 750px;"/></p></li>
</ul>
<div class="section" id="solving-svms-with-lagrange-multipliers">
<h4>Solving SVMs with Lagrange Multipliers<a class="headerlink" href="#solving-svms-with-lagrange-multipliers" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Imagine a hyperplane (green) <span class="math notranslate nohighlight">\(y= \sum_1^p \mathbf{w}_i * \mathbf{x}_i + w_0\)</span> that has slope <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, value ‘+1’ for the positive (red) support vectors, and ‘-1’ for the negative (blue) ones</p>
<ul>
<li><p>Margin between the boundary and support vectors is <span class="math notranslate nohighlight">\(\frac{y-w_0}{||\mathbf{w}||}\)</span>, with <span class="math notranslate nohighlight">\(||\mathbf{w}|| = \sum_i^p w_i^2\)</span></p></li>
<li><p>We want to find the weights that maximize <span class="math notranslate nohighlight">\(\frac{1}{||\mathbf{w}||}\)</span>. We can also do that by maximizing <span class="math notranslate nohighlight">\(\frac{1}{||\mathbf{w}||^2}\)</span></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># we create 40 separable points</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">sY</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">20</span>

<span class="c1"># fit the model</span>
<span class="n">s_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">s_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sX</span><span class="p">,</span> <span class="n">sY</span><span class="p">)</span>

<span class="nd">@interact</span>
<span class="k">def</span> <span class="nf">plot_svc_fit</span><span class="p">(</span><span class="n">rotationX</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">rotationY</span><span class="o">=</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
    <span class="c1"># get the separating hyperplane</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">s_clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">yy</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">-</span> <span class="p">(</span><span class="n">s_clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>

    <span class="c1"># plot the parallels to the separating hyperplane that pass through the</span>
    <span class="c1"># support vectors</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">s_clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">yy_down</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">s_clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">yy_up</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># plot the line, the points, and the nearest vectors to the plane</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mf">4.5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xx</span><span class="p">),</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_down</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xx</span><span class="p">),</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_up</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xx</span><span class="p">),</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">s_clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">s_clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">s_clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">85</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">sX</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sX</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">sX</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">c</span><span class="o">=</span><span class="n">sY</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>


    <span class="c1"># Planes</span>
    <span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">)</span>
    <span class="n">ZZ</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">XX</span><span class="o">+</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">YY</span><span class="o">+</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">XX</span><span class="o">*</span><span class="mi">0</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;XY plane&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;hyperplane&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">rotationX</span><span class="p">,</span> <span class="n">rotationY</span><span class="p">)</span> <span class="c1"># Use this to rotate the figure</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "2d2f86432cbc4d8e8143e42cac040962", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">interactive</span><span class="p">:</span>
    <span class="n">plot_svc_fit</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">135</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="geometric-interpretation">
<h5>Geometric interpretation<a class="headerlink" href="#geometric-interpretation" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><p>We want to maximize <span class="math notranslate nohighlight">\(f = \frac{1}{||w||^2}\)</span> (blue contours)</p></li>
<li><p>The hyperplane (red) must be <span class="math notranslate nohighlight">\(&gt; 1\)</span> for all positive examples:<br />
<span class="math notranslate nohighlight">\(g(\mathbf{w}) = \mathbf{w} \mathbf{x_i} + w_0 &gt; 1 \,\,\, \forall{i}, y(i)=1\)</span></p></li>
<li><p>Find the weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that satify <span class="math notranslate nohighlight">\(g\)</span> but maximize <span class="math notranslate nohighlight">\(f\)</span></p></li>
</ul>
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/LagrangeMultipliers3D.png" alt="ml" style="width: 650px;"/></div>
<div class="section" id="solution">
<h5>Solution<a class="headerlink" href="#solution" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><p>A quadratic loss function with linear constraints can be solved with <em>Lagrangian multipliers</em></p></li>
<li><p>This works by assigning a weight <span class="math notranslate nohighlight">\(a_i\)</span> (called a dual coefficient) to every data point <span class="math notranslate nohighlight">\(x_i\)</span></p>
<ul>
<li><p>They reflect how much individual points influence the weights <span class="math notranslate nohighlight">\(\mathbf{w}\)</span></p></li>
<li><p>The points with non-zero <span class="math notranslate nohighlight">\(a_i\)</span> are the <em>support vectors</em></p></li>
</ul>
</li>
<li><p>Next, solve the following <strong>Primal</strong> objective:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(y_i=\pm1\)</span> is the correct class for example <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{Primal} = \frac{1}{2} ||\mathbf{w}||^2 - \sum_{i=1}^{n} a_i y_i (\mathbf{w} \mathbf{x_i}  + w_0) + \sum_{i=1}^{n} a_i \]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[ \mathbf{w} = \sum_{i=1}^{n} a_i y_i \mathbf{x_i} \]</div>
<div class="math notranslate nohighlight">
\[ a_i \geq 0 \quad \text{and} \quad \sum_{i=1}^{l} a_i y_i = 0 \]</div>
<ul class="simple">
<li><p>It has a <strong>Dual</strong> formulation as well (See ‘Elements of Statistical Learning’ for the derivation):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{Dual} = \sum_{i=1}^{l} a_i - \frac{1}{2} \sum_{i,j=1}^{l} a_i a_j y_i y_j (\mathbf{x_i} \mathbf{x_j}) \]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[ a_i \geq 0 \quad \text{and} \quad \sum_{i=1}^{l} a_i y_i = 0 \]</div>
<ul class="simple">
<li><p>Computes the dual coefficients directly. A number <span class="math notranslate nohighlight">\(l\)</span> of these are non-zero (sparseness).</p>
<ul>
<li><p>Dot product <span class="math notranslate nohighlight">\(\mathbf{x_i} \mathbf{x_j}\)</span> can be interpreted as the closeness between points <span class="math notranslate nohighlight">\(\mathbf{x_i}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x_j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_{Dual}\)</span> increases if nearby support vectors <span class="math notranslate nohighlight">\(\mathbf{x_i}\)</span> with high weights <span class="math notranslate nohighlight">\(a_i\)</span> have different class <span class="math notranslate nohighlight">\(y_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}_{Dual}\)</span> also increases with the number of support vectors <span class="math notranslate nohighlight">\(l\)</span> and their weights <span class="math notranslate nohighlight">\(a_i\)</span></p></li>
</ul>
</li>
<li><p>Can be solved with quadratic programming, e.g. Sequential Minimal Optimization (SMO)</p></li>
</ul>
<p>Example result. The circled samples are support vectors, together with their coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># Plot SVM support vectors</span>
<span class="k">def</span> <span class="nf">plot_linear_svm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">ax</span><span class="p">):</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># get the separating hyperplane</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">yy</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">-</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># plot the parallels to the separating hyperplane</span>
    <span class="n">yy_down</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">xx</span><span class="o">-</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">yy_up</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">xx</span><span class="o">-</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># plot the line, the points, and the nearest vectors to the plane</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;C = </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_down</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_up</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">85</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>

    <span class="c1"># Add coefficients</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">dual_coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">coef</span><span class="p">),</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">0.15</span><span class="p">,</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>


<span class="c1"># we create 40 separable points</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">svm_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">svm_Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">20</span>
<span class="n">svm_fig</span><span class="p">,</span> <span class="n">svm_ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plot_linear_svm</span><span class="p">(</span><span class="n">svm_X</span><span class="p">,</span><span class="n">svm_Y</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">svm_ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_85_0.png" src="../_images/02 - Linear Models_85_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="making-predictions">
<h4>Making predictions<a class="headerlink" href="#making-predictions" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a_i\)</span> will be <em>0</em> if the training point lies on the right side of the decision boundary and outside the margin</p></li>
<li><p>The training samples for which <span class="math notranslate nohighlight">\(a_i\)</span> is not 0 are the <em>support vectors</em></p></li>
<li><p>Hence, the SVM model is completely defined by the support vectors and their dual coefficients (weights)</p></li>
<li><p>Knowing the dual coefficients <span class="math notranslate nohighlight">\(a_i\)</span>, we can find the weights <span class="math notranslate nohighlight">\(w\)</span> for the maximal margin separating hyperplane:<br />
$<span class="math notranslate nohighlight">\( \mathbf{w} = \sum_{i=1}^{l} a_i y_i \mathbf{x_i} \)</span>$</p></li>
<li><p>Hence, we can classify a new sample <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> by looking at the sign of <span class="math notranslate nohighlight">\(\mathbf{w}\mathbf{u}+w_0\)</span></p></li>
</ul>
<div class="section" id="svms-and-knn">
<h5>SVMs and kNN<a class="headerlink" href="#svms-and-knn" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><p>Remember, we will classify a new point <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> by looking at the sign of:<br />
$<span class="math notranslate nohighlight">\(f(x) = \mathbf{w}\mathbf{u}+w_0 = \sum_{i=1}^{l} a_i y_i \mathbf{x_i}\mathbf{u}+w_0\)</span>$</p></li>
<li><p><em>Weighted k-nearest neighbor</em> is a generalization of the k-nearest neighbor classifier. It classifies points by evaluating:<br />
$<span class="math notranslate nohighlight">\(f(x) = \sum_{i=1}^{k} a_i y_i dist(x_i, u)^{-1}\)</span>$</p></li>
<li><p>Hence: SVM’s predict much the same way as k-NN, only:</p>
<ul>
<li><p>They only consider the truly important points (the support vectors): <em>much</em> faster</p>
<ul>
<li><p>The number of neighbors is the number of support vectors</p></li>
</ul>
</li>
<li><p>The distance function is an <em>inner product of the inputs</em></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="regularized-soft-margin-svms">
<h4>Regularized (soft margin) SVMs<a class="headerlink" href="#regularized-soft-margin-svms" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>If the data is not linearly separable, (hard) margin maximization becomes meaningless</p></li>
<li><p>Relax the contraint by allowing an error <span class="math notranslate nohighlight">\(\xi_{i}\)</span>: <span class="math notranslate nohighlight">\(y_i (\mathbf{w}\mathbf{x_i} + w_0) \geq 1 - \xi_{i}\)</span></p></li>
<li><p>Or (since <span class="math notranslate nohighlight">\(\xi_{i} \geq 0\)</span>):
$<span class="math notranslate nohighlight">\(\xi_{i} =  max(0,1-y_i\cdot(\mathbf{w}\mathbf{x_i} + w_0))\)</span>$</p></li>
<li><p>The sum over all points is called <em>hinge loss</em>: <span class="math notranslate nohighlight">\(\sum_i^n \xi_{i}\)</span></p></li>
<li><p>Attenuating the error component with a hyperparameter <span class="math notranslate nohighlight">\(C\)</span>, we get the objective
$<span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{w}) = ||\mathbf{w}||^2 + C \sum_i^n \xi_{i}\)</span>$</p></li>
<li><p>Can still be solved with quadratic programming</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">yHat</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="o">+</span><span class="n">yHat</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 1&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 0&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Prediction value $\hat</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Hinge loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_89_0.png" src="../_images/02 - Linear Models_89_0.png" />
</div>
</div>
</div>
<div class="section" id="least-squares-svms">
<h4>Least Squares SVMs<a class="headerlink" href="#least-squares-svms" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>We can also use the <em>squares</em> of all the errors, or squared hinge loss: <span class="math notranslate nohighlight">\(\sum_i^n \xi_{i}^2\)</span></p></li>
<li><p>This yields the Least Squares SVM objective
$<span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{w}) = ||\mathbf{w}||^2 + C \sum_i^n \xi_{i}^2\)</span>$</p></li>
<li><p>Can be solved with Lagrangian Multipliers and a set of linear equations</p>
<ul>
<li><p>Still yields support vectors and still allows kernelization</p></li>
<li><p>Support vectors are not sparse, but pruning techniques exist</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span> <span class="mi">2</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 1&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">**</span> <span class="mi">2</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 0&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Prediction value $\hat</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Squared hinge loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_91_0.png" src="../_images/02 - Linear Models_91_0.png" />
</div>
</div>
</div>
<div class="section" id="effect-of-regularization-on-margin-and-support-vectors">
<h4>Effect of regularization on margin and support vectors<a class="headerlink" href="#effect-of-regularization-on-margin-and-support-vectors" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>SVM’s Hinge loss acts like L1 regularization, yields sparse models</p></li>
<li><p>C is the <em>inverse</em> regularization strength (inverse of <span class="math notranslate nohighlight">\(\alpha\)</span> in Lasso)</p>
<ul>
<li><p>Larger C: fewer support vectors, smaller margin, more overfitting</p></li>
<li><p>Smaller C: more support vectors, wider margin, less overfitting</p></li>
</ul>
</li>
<li><p>Needs to be tuned carefully to the data</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">svm_axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plot_linear_svm</span><span class="p">(</span><span class="n">svm_X</span><span class="p">,</span><span class="n">svm_Y</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">svm_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_linear_svm</span><span class="p">(</span><span class="n">svm_X</span><span class="p">,</span><span class="n">svm_Y</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span><span class="n">svm_axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_93_0.png" src="../_images/02 - Linear Models_93_0.png" />
</div>
</div>
<p>Same for non-linearly separable data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svm_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">svm_axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plot_linear_svm</span><span class="p">(</span><span class="n">svm_X</span><span class="p">,</span><span class="n">svm_Y</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">svm_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_linear_svm</span><span class="p">(</span><span class="n">svm_X</span><span class="p">,</span><span class="n">svm_Y</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span><span class="n">svm_axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_95_0.png" src="../_images/02 - Linear Models_95_0.png" />
</div>
</div>
<p>Large C values can lead to overfitting (e.g. fitting noise), small values can lead to underfitting</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_linear_svc_regularization</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_97_0.png" src="../_images/02 - Linear Models_97_0.png" />
</div>
</div>
</div>
<div class="section" id="svms-in-scikit-learn">
<h4>SVMs in scikit-learn<a class="headerlink" href="#svms-in-scikit-learn" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">svm.LinearSVC</span></code>: faster for large datasets</p>
<ul>
<li><p>Allows choosing between the primal or dual. Primal recommended when <span class="math notranslate nohighlight">\(n\)</span> &gt;&gt; <span class="math notranslate nohighlight">\(p\)</span></p></li>
<li><p>Returns <code class="docutils literal notranslate"><span class="pre">coef_</span></code> (<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>) and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> (<span class="math notranslate nohighlight">\(w_0\)</span>)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">svm.SVC</span></code> with <code class="docutils literal notranslate"><span class="pre">kernel=linear</span></code>: allows <em>kernelization</em> (see later)</p>
<ul>
<li><p>Also returns <code class="docutils literal notranslate"><span class="pre">support_vectors_</span></code> (the support vectors) and the <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code> <span class="math notranslate nohighlight">\(a_i\)</span></p></li>
<li><p>Scales at least quadratically with the number of samples <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">svm.LinearSVR</span></code> and <code class="docutils literal notranslate"><span class="pre">svm.SVR</span></code> are variants for regression</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Support vectors:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">dual_coef_</span><span class="p">[:])</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="c1"># Linearly separable dat</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">20</span>

<span class="c1"># Fit the model</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Get the support vectors and weights</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Support vectors:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">dual_coef_</span><span class="p">[:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Support vectors:
[[-1.021  0.241]
 [-0.467 -0.531]
 [ 0.951  0.58 ]]
Coefficients:
[[-0.048 -0.569  0.617]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="solving-svms-with-gradient-descent">
<h4>Solving SVMs with Gradient Descent<a class="headerlink" href="#solving-svms-with-gradient-descent" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Soft-margin SVMs can, alternatively, be solved using gradient decent</p>
<ul>
<li><p>Good for large datasets, but does not yield support vectors or kernelization</p></li>
</ul>
</li>
<li><p>Squared Hinge is differentiable</p></li>
<li><p>Hinge is not differentiable but convex, and has a subgradient:
$<span class="math notranslate nohighlight">\(\mathcal{L_{Hinge}}(\mathbf{w}) =  max(0,1-y_i (\mathbf{w}\mathbf{x_i} + w_0))\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L_{Hinge}}}{\partial w_i} =  \begin{cases}-y_i x_i &amp; y_i (\mathbf{w}\mathbf{x_i} + w_0) &lt; 1\\ 0 &amp; \text{otherwise} \\ \end{cases}\)</span>$</p></li>
<li><p>Can be solved with (stochastic) gradient descent</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 1&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true label = 0&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Prediction value $\hat</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Hinge loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_101_0.png" src="../_images/02 - Linear Models_101_0.png" />
</div>
</div>
</div>
<div class="section" id="generalized-svms">
<h4>Generalized SVMs<a class="headerlink" href="#generalized-svms" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Because the derivative of hinge loss is undefined at y=1, smoothed versions are often used:</p>
<ul>
<li><p>Squared hinge loss: yields <em>least squares SVM</em></p>
<ul>
<li><p>Equivalent to Ridge classification (with different solver)</p></li>
</ul>
</li>
<li><p>Modified Huber loss: squared hinge, but linear after -1. Robust against outliers</p></li>
</ul>
</li>
<li><p>Log loss can also be used (equivalent to logistic regression)</p></li>
<li><p>In sklearn, <code class="docutils literal notranslate"><span class="pre">SGDClassifier</span></code> can be used with any of these. Good for large datasets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">modified_huber_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">*</span> <span class="n">y_true</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span> <span class="o">*</span> <span class="n">z</span>
    <span class="n">loss</span><span class="p">[</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z</span><span class="p">[</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">loss</span><span class="p">[</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="mf">1.</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Zero-one loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">xx</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">xx</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Hinge loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;yellowgreen&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Perceptron loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">xx</span><span class="p">)),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Log loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">xx</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">xx</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;c-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Squared hinge loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">modified_huber_loss</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorchid&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span>
         <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modified Huber loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Decision function $f(x)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$Loss(y=1, f(x))$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_103_0.png" src="../_images/02 - Linear Models_103_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="perceptron">
<h3>Perceptron<a class="headerlink" href="#perceptron" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Represents a single neuron (node) with inputs <span class="math notranslate nohighlight">\(x_i\)</span>, a bias <span class="math notranslate nohighlight">\(w_0\)</span>, and output <span class="math notranslate nohighlight">\(y\)</span></p></li>
<li><p>Each connection has a (synaptic) weight <span class="math notranslate nohighlight">\(w_i\)</span>. The node outputs <span class="math notranslate nohighlight">\(\hat{y} = \sum_{i}^n x_{i}w_i + w_0\)</span></p></li>
<li><p>The <em>activation function</em> predicts 1 if <span class="math notranslate nohighlight">\(\mathbf{xw} + w_0 &gt; 0\)</span>, -1 otherwise</p></li>
<li><p>Weights can be learned with (stochastic) gradient descent and Hinge(0) loss</p>
<ul>
<li><p>Updated <em>only</em> on misclassification, corrects output by <span class="math notranslate nohighlight">\(\pm1\)</span>
$<span class="math notranslate nohighlight">\(\mathcal{L}_{Perceptron} = max(0,-y_i (\mathbf{w}\mathbf{x_i} + w_0))\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L_{Perceptron}}}{\partial w_i} =  \begin{cases}-y_i x_i &amp; y_i (\mathbf{w}\mathbf{x_i} + w_0) &lt; 0\\ 0 &amp; \text{otherwise} \\ \end{cases}\)</span>$
<img src="https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/perceptron.png" alt="ml" style="margin: 0 auto; width: 500px;"/></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="linear-models-for-multiclass-classification">
<h2>Linear Models for multiclass classification<a class="headerlink" href="#linear-models-for-multiclass-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="one-vs-rest-aka-one-vs-all">
<h3>one-vs-rest (aka one-vs-all)<a class="headerlink" href="#one-vs-rest-aka-one-vs-all" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Learn a binary model for each class vs. all other classes</p></li>
<li><p>Create as many binary models as there are classes</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">linear_svm</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">,</span><span class="mi">5</span><span class="o">*</span><span class="n">fig_scale</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">linear_svm</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span>
                                  <span class="n">mglearn</span><span class="o">.</span><span class="n">cm3</span><span class="o">.</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 2&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 1&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Line class 2&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_106_0.png" src="../_images/02 - Linear Models_106_0.png" />
</div>
</div>
<ul class="simple">
<li><p>Every binary classifiers makes a prediction, the one with the highest score (&gt;0) wins</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_classification</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.7</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">linear_svm</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span>
                                  <span class="n">mglearn</span><span class="o">.</span><span class="n">cm3</span><span class="o">.</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 2&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 1&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Line class 2&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/02 - Linear Models_108_0.png" src="../_images/02 - Linear Models_108_0.png" />
</div>
</div>
</div>
<div class="section" id="one-vs-one">
<h3>one-vs-one<a class="headerlink" href="#one-vs-one" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>An alternative is to learn a binary model for every <em>combination</em> of two classes</p>
<ul>
<li><p>For <span class="math notranslate nohighlight">\(C\)</span> classes, this results in <span class="math notranslate nohighlight">\(\frac{C(C-1)}{2}\)</span> binary models</p></li>
<li><p>Each point is classified according to a majority vote amongst all models</p></li>
<li><p>Can also be a ‘soft vote’: sum up the probabilities (or decision values) for all models. The class with the highest sum wins.</p></li>
</ul>
</li>
<li><p>Requires more models than one-vs-rest, but training each one is faster</p>
<ul>
<li><p>Only the examples of 2 classes are included in the training data</p></li>
</ul>
</li>
<li><p>Recommended for algorithms than learn well on small datasets</p>
<ul>
<li><p>Especially SVMs and Gaussian Processes</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%HTML</span>
<span class="p">&lt;</span><span class="nt">style</span><span class="p">&gt;</span><span class="w"></span>
<span class="nt">td</span><span class="w"> </span><span class="p">{</span><span class="k">font-size</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="kt">px</span><span class="p">}</span><span class="w"></span>
<span class="nt">th</span><span class="w"> </span><span class="p">{</span><span class="k">font-size</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="kt">px</span><span class="p">}</span><span class="w"></span>
<span class="p">.</span><span class="nc">rendered_html</span><span class="w"> </span><span class="nt">table</span><span class="o">,</span><span class="w"> </span><span class="p">.</span><span class="nc">rendered_html</span><span class="w"> </span><span class="nt">td</span><span class="o">,</span><span class="w"> </span><span class="p">.</span><span class="nc">rendered_html</span><span class="w"> </span><span class="nt">th</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">font-size</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="kt">px</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="p">&lt;/</span><span class="nt">style</span><span class="p">&gt;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
td {font-size: 16px}
th {font-size: 16px}
.rendered_html table, .rendered_html td, .rendered_html th {
    font-size: 16px;
}
</style>
</div></div>
</div>
</div>
</div>
<div class="section" id="linear-models-overview">
<h2>Linear models overview<a class="headerlink" href="#linear-models-overview" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Representation</p></th>
<th class="head"><p>Loss function</p></th>
<th class="head"><p>Optimization</p></th>
<th class="head"><p>Regularization</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Least squares</p></td>
<td><p>Linear function (R)</p></td>
<td><p>SSE</p></td>
<td><p>CFS or SGD</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>Ridge</p></td>
<td><p>Linear function (R)</p></td>
<td><p>SSE + L2</p></td>
<td><p>CFS or SGD</p></td>
<td><p>L2 strength (<span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
<tr class="row-even"><td><p>Lasso</p></td>
<td><p>Linear function (R)</p></td>
<td><p>SSE + L1</p></td>
<td><p>Coordinate descent</p></td>
<td><p>L1 strength (<span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p>Elastic-Net</p></td>
<td><p>Linear function (R)</p></td>
<td><p>SSE + L1 + L2</p></td>
<td><p>Coordinate descent</p></td>
<td><p><span class="math notranslate nohighlight">\(\alpha\)</span>, L1 ratio (<span class="math notranslate nohighlight">\(\rho\)</span>)</p></td>
</tr>
<tr class="row-even"><td><p>SGDRegressor</p></td>
<td><p>Linear function (R)</p></td>
<td><p>SSE, Huber, <span class="math notranslate nohighlight">\(\epsilon\)</span>-ins,… + L1/L2</p></td>
<td><p>SGD</p></td>
<td><p>L1/L2, <span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Logistic regression</p></td>
<td><p>Linear function (C)</p></td>
<td><p>Log + L1/L2</p></td>
<td><p>SGD, coordinate descent,…</p></td>
<td><p>L1/L2, <span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Ridge classification</p></td>
<td><p>Linear function (C)</p></td>
<td><p>SSE + L2</p></td>
<td><p>CFS or SGD</p></td>
<td><p>L2 strength (<span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p>Linear SVM</p></td>
<td><p>Support Vectors</p></td>
<td><p>Hinge(1)</p></td>
<td><p>Quadratic programming or SGD</p></td>
<td><p>Cost (C)</p></td>
</tr>
<tr class="row-even"><td><p>Least Squares SVM</p></td>
<td><p>Support Vectors</p></td>
<td><p>Squared Hinge</p></td>
<td><p>Linear equations or SGD</p></td>
<td><p>Cost (C)</p></td>
</tr>
<tr class="row-odd"><td><p>Perceptron</p></td>
<td><p>Linear function (C)</p></td>
<td><p>Hinge(0)</p></td>
<td><p>SGD</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-even"><td><p>SGDClassifier</p></td>
<td><p>Linear function (C)</p></td>
<td><p>Log, (Sq.) Hinge, Mod. Huber,…</p></td>
<td><p>SGD</p></td>
<td><p>L1/L2, <span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>SSE: Sum of Squared Errors</p></li>
<li><p>CFS: Closed-form solution</p></li>
<li><p>SGD: (Stochastic) Gradient Descent and variants</p></li>
<li><p>(R)egression, (C)lassification</p></li>
</ul>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Linear models</p>
<ul>
<li><p>Good for very large datasets (scalable)</p></li>
<li><p>Good for very high-dimensional data (not for low-dimensional data)</p></li>
</ul>
</li>
<li><p>Can be used to fit non-linear or low-dim patterns as well (see later)</p>
<ul>
<li><p>Preprocessing: e.g. Polynomial or Poisson transformations</p></li>
<li><p>Generalized linear models (kernelization)</p></li>
</ul>
</li>
<li><p>Regularization is important. Tune the regularization strength (<span class="math notranslate nohighlight">\(\alpha\)</span>)</p>
<ul>
<li><p>Ridge (L2): Good fit, sometimes sensitive to outliers</p></li>
<li><p>Lasso (L1): Sparse models: fewer features, more interpretable, faster</p></li>
<li><p>Elastic-Net: Trade-off between both, e.g. for correlated features</p></li>
</ul>
</li>
<li><p>Most can be solved by different optimizers (solvers)</p>
<ul>
<li><p>Closed form solutions or quadratic/linear solvers for smaller datasets</p></li>
<li><p>Gradient descent variants (SGD,CD,SAG,CG,…) for larger ones</p></li>
</ul>
</li>
<li><p>Multi-class classification can be done using a one-vs-all approach</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="01%20-%20Introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 1: Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="03%20-%20Kernelization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 3: Kernelization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Joaquin Vanschoren<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>