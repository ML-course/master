{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 4 Tutorial: Data engineering pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Auto-setup when running on Google Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install openml\n",
    "\n",
    "# General imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building Pipelines\n",
    "* In scikit-learn, a `pipeline` combines multiple processing _steps_ in a single estimator\n",
    "* All but the last step should be transformer (have a `transform` method)\n",
    "    * The last step can be a transformer too (e.g. Scaler+PCA)\n",
    "* It has a `fit`, `predict`, and `score` method, just like any other learning algorithm\n",
    "* Pipelines are built as a list of steps, which are (name, algorithm) tuples\n",
    "    * The name can be anything you want, but can't contain `'__'`\n",
    "    * We use `'__'` to refer to the hyperparameters, e.g. `svm__C`\n",
    "* Let's build, train, and score a `MinMaxScaler` + `LinearSVC` pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n",
    "pipe.fit(X_train, y_train).score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Now with cross-validation:\n",
    "``` python\n",
    "scores = cross_val_score(pipe, cancer.data, cancer.target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98245614 0.97368421 0.96491228 0.96491228 0.99115044]\n",
      "Average cross-validation score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipe, cancer.data, cancer.target)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We can retrieve the trained SVM by querying the right step indices\n",
    "``` python\n",
    "pipe.steps[1][1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM component: LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "print(\"SVM component: {}\".format(pipe.steps[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Or we can use the `named_steps` dictionary\n",
    "``` python\n",
    "pipe.named_steps['svm']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM component: LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM component: {}\".format(pipe.named_steps['svm']f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When you don't need specific names for specific steps, you can use `make_pipeline`\n",
    "    * Assigns names to steps automatically\n",
    "``` python\n",
    "pipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler()), ('linearsvc', LinearSVC(C=100))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualization of a pipeline `fit` and `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/07_pipelines.png\" alt=\"ml\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Pipelines in Grid-searches\n",
    "* We can use the pipeline as a single estimator in `cross_val_score` or `GridSearchCV`\n",
    "* To define a grid, refer to the hyperparameters of the steps\n",
    "    * Step `svm`, parameter `C` becomes `svm__C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy: 0.97\n",
      "Test set score: 0.97\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When we request the best estimator of the grid search, we'll get the best pipeline\n",
    "``` python\n",
    "grid.best_estimator_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('svm', SVC(C=10, gamma=1))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* And we can drill down to individual components and their properties\n",
    "``` python\n",
    "grid.best_estimator_.named_steps[\"svm\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM step:\n",
      "SVC(C=10, gamma=1)\n"
     ]
    }
   ],
   "source": [
    "# Get the SVM\n",
    "print(\"SVM step:\\n{}\".format(\n",
    "      grid.best_estimator_.named_steps[\"svm\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM support vector coefficients:\n",
      "[[ -1.39188844  -4.06940593  -0.435234    -0.70025696  -5.86542086\n",
      "   -0.41433994  -2.81390656 -10.         -10.          -3.41806527\n",
      "   -7.90768285  -0.16897821  -4.29887055  -1.13720135  -2.21362118\n",
      "   -0.19026766 -10.          -7.12847723 -10.          -0.52216852\n",
      "   -3.76624729  -0.01249056  -1.15920579 -10.          -0.51299862\n",
      "   -0.71224989 -10.          -1.50141938 -10.          10.\n",
      "    1.99516035   0.9094081    0.91913684   2.89650891   0.39896365\n",
      "   10.           9.81123374   0.4124202   10.          10.\n",
      "   10.           5.41518257   0.83036405   2.59337629   1.37050773\n",
      "   10.           0.27947936   1.55478824   6.58895182   1.48679571\n",
      "   10.           1.15559387   0.39055347   2.66341253   1.27687797\n",
      "    0.65127305   1.84096369   2.39518826   2.50425662]]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVM dual coefficients (support vector weights)\n",
    "print(\"SVM support vector coefficients:\\n{}\".format(\n",
    "      grid.best_estimator_.named_steps[\"svm\"].dual_coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid-searching preprocessing steps and model parameters\n",
    "* We can use grid search to optimize the hyperparameters of our preprocessing steps and learning algorithms at the same time\n",
    "* Consider the following pipeline:\n",
    "    - `StandardScaler`, without hyperparameters\n",
    "    - `PolynomialFeatures`, with the max. _degree_ of polynomials\n",
    "    - `Ridge` regression, with L2 regularization parameter _alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n",
    "                                                    random_state=0)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = pipeline.make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We don't know the optimal polynomial degree or alpha value, so we use a grid search (or random search) to find the optimal values\n",
    "``` python\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\n",
    "grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "# Note: I had to use n_jobs=1. (n_jobs=-1 stalls on my machine)\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Visualing the $R^2$ results as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n",
    "            vmin=0, cmap=\"viridis\")\n",
    "plt.xlabel(\"ridge__alpha\")\n",
    "plt.ylabel(\"polynomialfeatures__degree\")\n",
    "plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\n",
    "plt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n",
    "           param_grid['polynomialfeatures__degree'])\n",
    "\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Here, degree-2 polynomials help (but degree-3 ones don't), and tuning the alpha parameter helps as well.\n",
    "* Not using the polynomial features leads to suboptimal results (see the results for degree 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'polynomialfeatures__degree': 1, 'ridge__alpha': 10}\n",
      "Test-set score: 0.59\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnions\n",
    "- Sometimes you want to apply multiple preprocessing techniques and use the _combined_ produced features\n",
    "- Simply appending the produced features is called a `FeatureJoin`\n",
    "- Example: Apply both PCA and feature selection, and run an SVM on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined space has 3 features\n",
      "Pipeline(steps=[('features',\n",
      "                 FeatureUnion(transformer_list=[('pca', PCA(n_components=3)),\n",
      "                                                ('univ_select',\n",
      "                                                 SelectKBest(k=1))])),\n",
      "                ('svm', SVC(C=10, kernel='linear'))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "print(\"Combined space has\", X_features.shape[1], \"features\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "- A pipeline applies a transformer on _all_ columns\n",
    "    - If your dataset has both numeric and categorical features, you often want to apply different techniques on each\n",
    "    - You _could_ manually split up the dataset, and then feature-join the processed features (tedious)\n",
    "- `ColumnTransformer` allows you to specify on which columns a preprocessor has to be run\n",
    "    - Either by specifying the feature names, indices, or a binary mask\n",
    "- You can include multiple transformers in a ColumnTransformer\n",
    "    - In the end the results will be feature-joined\n",
    "    - Hence, the order of the features will change!\n",
    "        The features of the last transformer will be at the end\n",
    "- Each transformer can be a pipeline\n",
    "    - Handy if you need to apply multiple preprocessing steps on a set of features\n",
    "    - E.g. use a ColumnTransformer with one sub-pipeline for numerical features and one for categorical features.\n",
    "- In the end, the columntransformer can again be included as part of a pipeline\n",
    "    - E.g. to add a classfier and include the whole pipeline in a grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Handle a dataset (Titanic) with both categorical an numeric features\n",
    "- Numeric features: impute missing values and scale\n",
    "- Categorical features: Impute missing values and apply one-hot-encoding\n",
    "- Finally, run an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data from https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Alternatively X and y can be obtained directly from the frame attribute:\n",
    "# X = titanic.frame.drop('survived', axis=1)\n",
    "# y = titanic.frame['survived']\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can again run optimize any of the hyperparameters (preprocessing-related ones included) in a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.798\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
